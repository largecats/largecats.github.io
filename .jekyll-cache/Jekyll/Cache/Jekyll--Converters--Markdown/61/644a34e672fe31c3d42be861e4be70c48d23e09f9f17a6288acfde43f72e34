I"Jò<ul id="markdown-toc">
  <li><a href="#use-udfs-only-when-necessary" id="markdown-toc-use-udfs-only-when-necessary">Use udfs only when necessary</a></li>
  <li><a href="#debug-udfs-by-raising-exceptions" id="markdown-toc-debug-udfs-by-raising-exceptions">Debug udfs by raising exceptions</a>    <ul>
      <li><a href="#yarn-commands" id="markdown-toc-yarn-commands">Yarn commands</a></li>
      <li><a href="#raise-exceptions" id="markdown-toc-raise-exceptions">Raise exceptions</a></li>
      <li><a href="#return-message-with-output" id="markdown-toc-return-message-with-output">Return message with output</a></li>
    </ul>
  </li>
  <li><a href="#do-not-use-dataframe-objects-inside-udfs" id="markdown-toc-do-not-use-dataframe-objects-inside-udfs">Do not use DataFrame objects inside udfs</a></li>
  <li><a href="#do-not-import--define-udfs-before-creating-sparkcontext" id="markdown-toc-do-not-import--define-udfs-before-creating-sparkcontext">Do not import / define udfs before creating SparkContext</a></li>
  <li><a href="#defining-udfs-in-a-class" id="markdown-toc-defining-udfs-in-a-class">Defining udfs in a class</a></li>
</ul>

<p>User defined function (udf) is a feature in (Py)Spark that allows user to define customized functions with column arguments. This post summarizes some pitfalls when using udfs.</p>

<p>Example of udf:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># dataframe of channelids
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">udf</span><span class="p">,</span> <span class="n">lit</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">DoubleType</span><span class="p">,</span> <span class="n">BooleanType</span><span class="p">,</span> <span class="n">StringType</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SQLContext</span><span class="p">,</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SQLContext</span><span class="p">,</span> <span class="n">SparkConf</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># register udf
</span><span class="n">square_udf</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">DoubleType</span><span class="p">())</span>

<span class="c1"># sample dataframe
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">numbers</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">df_pd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'number'</span><span class="p">:</span> <span class="n">numbers</span><span class="p">})</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df_pd</span><span class="p">)</span>

<span class="c1"># udf usage
</span><span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">'squared'</span><span class="p">,</span> <span class="n">square_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'number'</span><span class="p">]))</span>
</code></pre></div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> df.show<span class="o">(</span>10<span class="o">)</span>
+------------------+
|            number|
+------------------+
| 5.685395575385293|
| 5.202460175305168|
| 5.631595697611453|
|3.5112203388890113|
|3.8605235152919115|
| 4.465308911421083|
|5.3378137934754335|
| 4.137082993915284|
|3.2010281920995576|
| 6.515478687298831|
+------------------+
only showing top 10 rows

<span class="o">&gt;&gt;&gt;</span> df1.show<span class="o">(</span>10<span class="o">)</span>
+------------------+------------------+
|            number|           squared|
+------------------+------------------+
| 5.685395575385293|32.323722848610664|
| 5.202460175305168| 27.06559187563628|
| 5.631595697611453|31.714870101355828|
|3.5112203388890113|12.328668268227863|
|3.8605235152919115|14.903641812121817|
| 4.465308911421083|19.938983674416537|
|5.3378137934754335|28.492256093816597|
| 4.137082993915284|17.115455698543048|
|3.2010281920995576|10.246581486616162|
| 6.515478687298831|  42.4514625246453|
+------------------+------------------+
only showing top 10 rows
</code></pre></div></div>
<p>I encountered the following pitfalls when using udfs.</p>

<h1 id="use-udfs-only-when-necessary">Use udfs only when necessary</h1>

<p>Spark optimizes native operations. One such optimization is <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Optimizer-PushDownPredicate.html">predicate pushdown</a>.</p>

<p>A predicate is a statement that is either true or false, e.g., <code class="language-plaintext highlighter-rouge">df.amount &gt; 0</code>. Conditions in <code class="language-plaintext highlighter-rouge">.where()</code> and <code class="language-plaintext highlighter-rouge">.filter()</code> are predicates. Predicate pushdown refers to the behavior that if the native <code class="language-plaintext highlighter-rouge">.where()</code> or <code class="language-plaintext highlighter-rouge">.filter()</code> are used after loading a dataframe, Spark ‚Äúpushes‚Äù these operations down to the data source level to minimize the amount of data loaded. That is, it will filter then load instead of load then filter.</p>

<p>For udfs, no such optimization exists, as Spark will not and cannot optimize udfs.</p>

<p>Consider a dataframe of orders, individual items in the orders, the number, price, and weight of each item.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># example data
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span>
<span class="n">orderids</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">//</span><span class="mi">4</span><span class="p">)],</span> <span class="p">[])</span>
<span class="n">itemids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">n</span><span class="o">//</span><span class="mi">4</span><span class="p">)</span>
<span class="n">numbers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span> <span class="o">+</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="o">-</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">prices</span> <span class="o">=</span> <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)]</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)]</span>
<span class="n">df_pd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s">'orderid'</span><span class="p">:</span> <span class="n">orderids</span><span class="p">,</span>
    <span class="s">'itemid'</span><span class="p">:</span> <span class="n">itemids</span><span class="p">,</span>
    <span class="s">'number'</span><span class="p">:</span> <span class="n">numbers</span><span class="p">,</span>
    <span class="s">'price'</span><span class="p">:</span> <span class="n">prices</span><span class="p">,</span>
    <span class="s">'weight'</span><span class="p">:</span> <span class="n">weights</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df_pd</span><span class="p">).</span><span class="n">select</span><span class="p">(</span><span class="s">'orderid'</span><span class="p">,</span> <span class="s">'itemid'</span><span class="p">,</span> <span class="s">'number'</span><span class="p">,</span> <span class="s">'price'</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">'/user/xiaolinfan/demo/df.parquet'</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'overwrite'</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> df.show<span class="o">(</span>10<span class="o">)</span>
19/10/29 22:30:46 WARN TaskSetManager: Stage 6 contains a task of very large size <span class="o">(</span>381 KB<span class="o">)</span><span class="nb">.</span> The maximum recommended task size is 100 KB.
+-------+------+------+------+------+
|orderid|itemid|number| price|weight|
+-------+------+------+------+------+
|      0|     0|     0|105.43|  4.54|
|      0|     1|     0|131.41|  4.88|
|      0|     2|     0|114.43|  6.81|
|      0|     3|     0|141.96|  4.81|
|      1|     0|     0| 94.54|  4.28|
|      1|     1|     0| 93.35|  3.71|
|      1|     2|     0| 82.85|  6.29|
|      1|     3|     0|118.56|  5.15|
|      2|     0|     0|110.24|  4.55|
|      2|     1|     0|103.58|  5.58|
+-------+------+------+------+------+
only showing top 10 rows
</code></pre></div></div>
<p>Consider reading in the dataframe and selecting only those rows with <code class="language-plaintext highlighter-rouge">df.number &gt; 0</code>. Observe the predicate pushdown optimization in the physical plan, as shown by <code class="language-plaintext highlighter-rouge">PushedFilters: [IsNotNull(number), GreaterThan(number,0)]</code>.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nb">df</span> <span class="o">=</span> spark.read.parquet<span class="o">(</span><span class="s1">'/user/xiaolinfan/demo/df.parquet'</span><span class="o">)</span>
<span class="o">&gt;&gt;&gt;</span> df1 <span class="o">=</span> df.filter<span class="o">(</span>df.number <span class="o">&gt;</span> 0<span class="o">)</span>
<span class="o">&gt;&gt;&gt;</span> df1.explain<span class="o">()</span>
<span class="o">==</span> Physical Plan <span class="o">==</span>
<span class="k">*</span><span class="o">(</span>2<span class="o">)</span> HashAggregate<span class="o">(</span><span class="nv">keys</span><span class="o">=[</span>number#72L, weight#74, price#73, itemid#71L, orderid#70L], <span class="nv">functions</span><span class="o">=[])</span>
+- Exchange hashpartitioning<span class="o">(</span>number#72L, weight#74, price#73, itemid#71L, orderid#70L, 200<span class="o">)</span>
   +- <span class="k">*</span><span class="o">(</span>1<span class="o">)</span> HashAggregate<span class="o">(</span><span class="nv">keys</span><span class="o">=[</span>number#72L, weight#74, price#73, itemid#71L, orderid#70L], <span class="nv">functions</span><span class="o">=[])</span>
      +- <span class="k">*</span><span class="o">(</span>1<span class="o">)</span> Project <span class="o">[</span>orderid#70L, itemid#71L, number#72L, price#73, weight#74]
         +- <span class="k">*</span><span class="o">(</span>1<span class="o">)</span> Filter <span class="o">(</span>isnotnull<span class="o">(</span>number#72L<span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="o">(</span>number#72L <span class="o">&gt;</span> 0<span class="o">))</span>
            +- <span class="k">*</span><span class="o">(</span>1<span class="o">)</span> FileScan parquet <span class="o">[</span>orderid#70L,itemid#71L,number#72L,price#73,weight#74] Batched: <span class="nb">true</span>, Format: Parquet, Location: InMemoryFileIndex[hdfs://localhost:9000/user/xiaolinfan/demo/df.parquet], PartitionFilters: <span class="o">[]</span>, PushedFilters: <span class="o">[</span>IsNotNull<span class="o">(</span>number<span class="o">)</span>, GreaterThan<span class="o">(</span>number,0<span class="o">)]</span>, ReadSchema: struct&lt;orderid:bigint,itemid:bigint,number:bigint,price:double,weight:double&gt;
</code></pre></div></div>
<p>Now, instead of <code class="language-plaintext highlighter-rouge">df.number &gt; 0</code>, use a <code class="language-plaintext highlighter-rouge">filter_udf</code> as the predicate. Observe that there is no longer predicate pushdown in the physical plan, as shown by <code class="language-plaintext highlighter-rouge">PushedFilters: []</code>.</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> filter_udf <span class="o">=</span> udf<span class="o">(</span>lambda x: x <span class="o">&gt;</span> 0, BooleanType<span class="o">())</span>
<span class="o">&gt;&gt;&gt;</span> df2 <span class="o">=</span> df.filter<span class="o">(</span>filter_udf<span class="o">(</span><span class="nb">df</span><span class="o">[</span><span class="s1">'number'</span><span class="o">]))</span>
<span class="o">&gt;&gt;&gt;</span> df2.explain<span class="o">()</span>
<span class="o">==</span> Physical Plan <span class="o">==</span>
<span class="k">*</span><span class="o">(</span>3<span class="o">)</span> HashAggregate<span class="o">(</span><span class="nv">keys</span><span class="o">=[</span>number#72L, weight#74, price#73, itemid#71L, orderid#70L], <span class="nv">functions</span><span class="o">=[])</span>
+- Exchange hashpartitioning<span class="o">(</span>number#72L, weight#74, price#73, itemid#71L, orderid#70L, 200<span class="o">)</span>
   +- <span class="k">*</span><span class="o">(</span>2<span class="o">)</span> HashAggregate<span class="o">(</span><span class="nv">keys</span><span class="o">=[</span>number#72L, weight#74, price#73, itemid#71L, orderid#70L], <span class="nv">functions</span><span class="o">=[])</span>
      +- <span class="k">*</span><span class="o">(</span>2<span class="o">)</span> Project <span class="o">[</span>orderid#70L, itemid#71L, number#72L, price#73, weight#74]
         +- <span class="k">*</span><span class="o">(</span>2<span class="o">)</span> Filter pythonUDF0#81: boolean
            +- BatchEvalPython <span class="o">[</span>&lt;lambda&gt;<span class="o">(</span>number#72L<span class="o">)]</span>, <span class="o">[</span>orderid#70L, itemid#71L, number#72L, price#73, weight#74, pythonUDF0#81]
               +- <span class="k">*</span><span class="o">(</span>1<span class="o">)</span> FileScan parquet <span class="o">[</span>orderid#70L,itemid#71L,number#72L,price#73,weight#74] Batched: <span class="nb">true</span>, Format: Parquet, Location: InMemoryFileIndex[hdfs://localhost:9000/user/xiaolinfan/demo/df.parquet], PartitionFilters: <span class="o">[]</span>, PushedFilters: <span class="o">[]</span>, ReadSchema: struct&lt;orderid:bigint,itemid:bigint,number:bigint,price:double,weight:double&gt;
</code></pre></div></div>

<div style="text-align: center"><img src="/images/time_compare.png" width="600px" /></div>
<div align="center">
<sup>Frequency distribution of execution times of df1.count() and df2.count(), with sample size 30.</sup>
</div>

<h1 id="debug-udfs-by-raising-exceptions">Debug udfs by raising exceptions</h1>

<p>Programs are usually debugged by raising exceptions, inserting breakpoints (e.g., using debugger), or quick printing/logging.</p>

<p>Debugging (Py)Spark udfs requires some special handling.</p>

<p>Consider the same sample dataframe created before. Suppose we want to calculate the total price and weight of each item in the orders via the udfs <code class="language-plaintext highlighter-rouge">get_item_price_udf()</code> and <code class="language-plaintext highlighter-rouge">get_item_weight_udf()</code>. Suppose further that we want to print the number and price of the item if the total item price is no greater than <code class="language-plaintext highlighter-rouge">0</code>. (We use printing instead of logging as an example because logging from Pyspark requires further configurations, see <a href="https://stackoverflow.com/questions/25407550/how-do-i-log-from-my-python-spark-script">here</a>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># example data
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">'/user/xiaolinfan/demo/df.parquet'</span><span class="p">)</span>

<span class="c1"># debugging
</span><span class="k">def</span> <span class="nf">get_item_price</span><span class="p">(</span><span class="n">number</span><span class="p">,</span> <span class="n">price</span><span class="p">):</span>
    <span class="n">item_price</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">number</span> <span class="o">*</span> <span class="n">price</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">item_price</span> <span class="o">&lt;=</span> <span class="mf">0.</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s">'number = {}, price = {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">number</span><span class="p">,</span> <span class="n">price</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">item_price</span>

<span class="k">def</span> <span class="nf">get_item_weight</span><span class="p">(</span><span class="n">number</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="n">item_weight</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">number</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">item_weight</span>

<span class="n">get_item_price_udf</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="n">get_item_price</span><span class="p">,</span> <span class="n">DoubleType</span><span class="p">())</span>
<span class="n">get_item_weight_udf</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="n">get_item_weight</span><span class="p">,</span> <span class="n">DoubleType</span><span class="p">())</span>

<span class="n">df_item</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">'item_price'</span><span class="p">,</span> <span class="n">get_item_price_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'number'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'price'</span><span class="p">]))</span>\
    <span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">'item_weight'</span><span class="p">,</span> <span class="n">get_item_price_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'number'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'weight'</span><span class="p">]))</span>
<span class="n">df_item</span><span class="p">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="n">df_item</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Submitting this script via <code class="language-plaintext highlighter-rouge">spark-submit --master yarn</code> generates the following output. Observe that the the first <code class="language-plaintext highlighter-rouge">10</code> rows of the dataframe have <code class="language-plaintext highlighter-rouge">item_price == 0.0</code>, and the <code class="language-plaintext highlighter-rouge">.show()</code> command computes the first <code class="language-plaintext highlighter-rouge">20</code> rows of the dataframe, so we expect the <code class="language-plaintext highlighter-rouge">print()</code> statements in <code class="language-plaintext highlighter-rouge">get_item_price_udf()</code> to be executed. However, they are not printed to the console.</p>
<div style="text-align: center"><img src="/images/udf_print_not_shown.png" width="600px" /></div>
<div align="center">
<sup>Print statements inside udfs are not shown in console.</sup>
</div>

<p>This can be explained by the nature of distributed execution in Spark (see <a href="https://www.oreilly.com/library/view/learning-spark/9781449359034/">here</a>). In short, objects are defined in driver program but are executed at worker nodes (or executors). In particular, udfs are executed at executors. Thus, in order to see the <code class="language-plaintext highlighter-rouge">print()</code> statements inside udfs, we need to view the executor logs.</p>
<div style="text-align: center"><img src="/images/distributed_execution.png" width="600px" /></div>
<div align="center">
<sup>Spark's distributed execution, taken from [here](https://www.oreilly.com/library/view/learning-spark/9781449359034/).</sup>
</div>

<p>Another way to validate this is to observe that if we submit the spark job in standalone mode without distributed execution, we can directly see the udf <code class="language-plaintext highlighter-rouge">print()</code> statements in the console:</p>
<div style="text-align: center"><img src="/images/standalone.png" width="400px" /></div>
<div align="center">
<sup>Running in standalone mode without distributed execution.</sup>
</div>

<p>There are a few workarounds.</p>

<h2 id="yarn-commands">Yarn commands</h2>
<ol>
  <li>Enable yarn log aggregation by setting</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;property&gt;
        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
</code></pre></div></div>

<p>in <code class="language-plaintext highlighter-rouge">yarn-site.xml</code> in <code class="language-plaintext highlighter-rouge">$HADOOP_HOME/etc/hadoop/</code>.</p>
<ol>
  <li>View executor logs via <code class="language-plaintext highlighter-rouge">yarn logs -applicationId &lt;application_id&gt;</code>, as instructed <a href="https://spark.apache.org/docs/latest/running-on-yarn.html#debugging-your-application">here</a>. <code class="language-plaintext highlighter-rouge">application_id</code> can be found in the resource manager UI</li>
</ol>
<div style="text-align: center"><img src="/images/application_id.png" width="800px" /></div>
<div align="center">
</div>
<p>or via the command <code class="language-plaintext highlighter-rouge">yarn application -list -appStates ALL</code> (<code class="language-plaintext highlighter-rouge">-appStates ALL</code> shows applications that are finished).</p>

<p><img src="/images/executor_log1.png" alt="" width="800px" /></p>

<p><img src="/images/executor_log2.png" alt="" width="800px" /></p>

<p>Note: To see that the above is the log of an executor and not the driver, can view the driver ip address at <code class="language-plaintext highlighter-rouge">yarn application -status &lt;application_id&gt;</code>. Usually, the container ending with <code class="language-plaintext highlighter-rouge">000001</code> is where the driver is run.</p>

<p>This method is straightforward, but requires access to yarn configurations. This could be not as straightforward if the production environment is not managed by the user.</p>

<h2 id="raise-exceptions">Raise exceptions</h2>

<p>Another way to show information from udf is to raise exceptions, e.g.,</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_item_price</span><span class="p">(</span><span class="n">number</span><span class="p">,</span> <span class="n">price</span><span class="p">):</span>
    <span class="n">item_price</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">number</span> <span class="o">*</span> <span class="n">price</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">item_price</span> <span class="o">&lt;=</span> <span class="mf">0.</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s">'number = {}, price = {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">number</span><span class="p">,</span> <span class="n">price</span><span class="p">)</span>
        <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">item_price</span>
</code></pre></div></div>
<div style="text-align: center"><img src="/images/raise_exception.png" width="800px" /></div>
<div align="center">
</div>

<p>This method is independent from production environment configurations. But the program does not continue after raising exception.</p>

<h2 id="return-message-with-output">Return message with output</h2>

<p>Yet another workaround is to wrap the message with the output, as suggested <a href="https://stackoverflow.com/questions/54252682/pyspark-udf-print-row-being-analyzed">here</a>, and then extract the real output afterwards.</p>

<h1 id="do-not-use-dataframe-objects-inside-udfs">Do not use DataFrame objects inside udfs</h1>

<p>Serialization is the process of turning an object into a format that can be stored/transmitted (e.g., byte stream) and reconstructed later.</p>

<p>E.g., serializing and deserializing trees:</p>
<div style="text-align: center"><img src="/images/serializeTree.jpg" width="400px" /></div>
<div align="center">
<sup>A tree object can be serialized into a stream of characters and deserialized back into the tree, taken from [here](https://www.geeksforgeeks.org/serialize-deserialize-binary-tree/).</sup>
</div>

<p>Because Spark uses distributed execution, objects defined in driver need to be sent to workers. This requires them to be serializable. In particular, udfs need to be serializable.</p>

<p>Consider a dataframe of orderids and channelids associated with the dataframe constructed previously. Suppose we want to add a column of channelids to the original dataframe. We do this via a udf <code class="language-plaintext highlighter-rouge">get_channelid_udf()</code> that returns a channelid given an orderid (this could be done with a join, but for the sake of giving an example, we use the udf).</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># dataframe of channelids
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">orderids</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="nb">sum</span><span class="p">([[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">//</span><span class="mi">4</span><span class="p">)],</span> <span class="p">[])))</span>
<span class="n">channelids</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)),</span><span class="nb">len</span><span class="p">(</span><span class="n">orderids</span><span class="p">))</span>
<span class="n">channel_df_pd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span><span class="s">'orderid'</span><span class="p">:</span> <span class="n">orderids</span><span class="p">,</span>
    <span class="s">'channelid'</span><span class="p">:</span> <span class="n">channelids</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">channel_df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">channel_df_pd</span><span class="p">).</span><span class="n">select</span><span class="p">(</span><span class="s">'orderid'</span><span class="p">,</span> <span class="s">'channelid'</span><span class="p">)</span>
<span class="n">channel_df</span><span class="p">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="n">channel_df</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">channel_df</span><span class="p">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">'channel_df'</span><span class="p">)</span>

<span class="c1"># get channelid from orderid
</span><span class="k">def</span> <span class="nf">get_channelid</span><span class="p">(</span><span class="n">orderid</span><span class="p">):</span>
    <span class="n">channelid</span> <span class="o">=</span> <span class="n">channel_df</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">channel_df</span><span class="p">.</span><span class="n">orderid</span> <span class="o">==</span> <span class="n">orderid</span><span class="p">).</span><span class="n">select</span><span class="p">(</span><span class="s">'channelid'</span><span class="p">).</span><span class="n">collect</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="s">'channelid'</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">channelid</span><span class="p">)</span>

<span class="c1"># register as udf
</span><span class="n">get_channelid_udf</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="n">get_channelid</span><span class="p">,</span> <span class="n">StringType</span><span class="p">())</span>

<span class="c1"># apply udf
</span><span class="n">df_channelid</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">'channelid'</span><span class="p">,</span> <span class="n">get_channelid_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'orderid'</span><span class="p">]))</span>
</code></pre></div></div>
<div style="text-align: center"><img src="/images/could_not_serialize.png" width="800px" /></div>
<div align="center">
<sup>Querying inside udf raises "could not serialize" exception.</sup>
</div>

<p>This is because the Spark context is not serializable. (Though it may be in the future, see <a href="https://databricks.com/session/streamsql-on-spark-manipulating-streams-by-sql-using-spark">here</a>.) Since udfs need to be serialized to be sent to the executors, a Spark context (e.g., dataframe, querying) inside an udf would raise the above error.</p>

<p>Some workarounds include:</p>
<ol>
  <li>If the query is simple, use join.</li>
  <li>If the query is too complex to use join and the dataframe is small enough to fit in memory, consider converting the Spark dataframe to Pandas dataframe via <code class="language-plaintext highlighter-rouge">.toPandas()</code> and perform query on the pandas dataframe object.</li>
  <li>If the object concerned is not a Spark context, consider implementing Java‚Äôs Serializable interface (e.g., in Scala, this would be <code class="language-plaintext highlighter-rouge">extend Serializable</code>).</li>
</ol>

<h1 id="do-not-import--define-udfs-before-creating-sparkcontext">Do not import / define udfs before creating SparkContext</h1>

<p>Spark udfs require <code class="language-plaintext highlighter-rouge">SparkContext</code> to work. So udfs must be defined or imported after having initialized a <code class="language-plaintext highlighter-rouge">SparkContext</code>. Otherwise, the Spark job will freeze, see <a href="https://stackoverflow.com/questions/35923775/functions-from-custom-module-not-working-in-pyspark-but-they-work-when-inputted">here</a>.</p>

<h1 id="defining-udfs-in-a-class">Defining udfs in a class</h1>
<p>If udfs are defined at top-level, they can be imported without errors.</p>

<p>If udfs need to be put in a class, they should be defined as attributes built from static methods of the class, e.g.,</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Function</span><span class="p">():</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">foo_udf</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">udf</span><span class="p">(</span><span class="n">Function</span><span class="p">.</span><span class="n">foo</span><span class="p">,</span> <span class="n">T</span><span class="p">.</span><span class="n">DoubleType</span><span class="p">())</span>
    
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">foo</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">.</span><span class="mi">5</span>
</code></pre></div></div>
<p>otherwise they may cause serialization errors.</p>

<p>An explanation is that only objects defined at top-level are serializable. These include udfs defined at top-level, attributes of a class defined at top-level, but not methods of that class (see <a href="https://stackoverflow.com/questions/58416527/pyspark-user-defined-functions-inside-of-a-class]">here</a>).</p>
:ET